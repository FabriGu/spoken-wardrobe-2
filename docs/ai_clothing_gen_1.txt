"""
PHASE 4: AI CLOTHING GENERATION WITH STABLE DIFFUSION INPAINTING
=================================================================

PURPOSE: Take the body part mask from Phase 3 and use Stable Diffusion Inpainting
to generate clothing directly on the person in the video frame.

LEARNING RESOURCES:
- SD Inpainting: https://huggingface.co/docs/diffusers/using-diffusers/inpaint
- Inpainting Pipeline: https://huggingface.co/runwayml/stable-diffusion-inpainting
- Prompt Engineering: https://huggingface.co/docs/diffusers/using-diffusers/write_prompts

WHAT YOU'RE BUILDING:
A system that takes:
1. Original video frame
2. Mask showing where clothing should go (from Phase 3)
3. Text prompt describing the clothing (from Phase 2)

And generates:
1. Inpainted image with clothing on the person
2. Cropped clothing PNG with transparency (for overlay on live video)

KEY DIFFERENCE FROM OLD APPROACH:
OLD: Generate clothing on white background → remove bg → warp to fit
NEW: Inpaint directly on the person using mask → crop to get clothing PNG
Much simpler and more natural results!
"""

import torch
from diffusers import StableDiffusionInpaintPipeline
from PIL import Image
import numpy as np
import cv2
import time
import os


class ClothingGenerator:
    """
    This class handles AI-powered clothing generation using Stable Diffusion Inpainting.
    
    HOW IT WORKS:
    1. Takes a video frame + mask from BodyPix
    2. Uses SD Inpainting to paint clothing in the masked area
    3. Extracts just the clothing using the mask
    4. Returns both full inpainted image and clothing PNG
    """
    
    def __init__(self, model_id="runwayml/stable-diffusion-inpainting", cache_dir="./models"):
        """
        Initialize the clothing generation system.
        
        PARAMETERS:
        - model_id: Which SD model to use
                   "runwayml/stable-diffusion-inpainting" is the official SD 1.5 inpainting model
        - cache_dir: Where to store downloaded model weights
        
        MODEL DOWNLOAD: ~5GB on first run (includes VAE, UNet, text encoder)
        """
        
        self.model_id = model_id
        self.cache_dir = cache_dir
        
        # Create cache directory
        os.makedirs(cache_dir, exist_ok=True)
        
        # Will be initialized when we load the model
        self.pipeline = None
        self.device = None
        
        # Generation parameters (can be tuned)
        self.num_inference_steps = 30  # More steps = higher quality but slower
        self.guidance_scale = 7.5  # How closely to follow the prompt (1-20)
        
        # Cache for generated images
        self.generation_cache = {}
        
        print("ClothingGenerator initialized")
        print(f"Model: {model_id}")
        print(f"Cache directory: {cache_dir}")
    
    
    def load_model(self):
        """
        Load the Stable Diffusion Inpainting model.
        
        WARNING: First run downloads ~5GB!
        This takes 2-5 minutes depending on internet speed.
        Subsequent runs load from cache in ~10 seconds.
        """
        
        print("\nLoading Stable Diffusion Inpainting model...")
        print("First time will download ~5GB (be patient!)")
        
        start_time = time.time()
        
        # Determine device
        if torch.backends.mps.is_available():
            self.device = "mps"  # Mac GPU
            print("Using Mac GPU (MPS) acceleration")
        elif torch.cuda.is_available():
            self.device = "cuda"  # Nvidia GPU
            print("Using NVIDIA GPU (CUDA) acceleration")
        else:
            self.device = "cpu"
            print("Using CPU (will be slow!)")
        
        # Load the inpainting pipeline
        # This is specifically designed for inpainting tasks
        self.pipeline = StableDiffusionInpaintPipeline.from_pretrained(
            self.model_id,
            torch_dtype=torch.float16 if self.device != "cpu" else torch.float32,
            cache_dir=self.cache_dir
        )
        
        # Move to device
        self.pipeline = self.pipeline.to(self.device)
        
        # Enable optimizations
        self.pipeline.enable_attention_slicing()
        
        # On Mac, additional memory optimization
        if self.device == "mps":
            self.pipeline.enable_attention_slicing("max")
        
        load_time = time.time() - start_time
        print(f"✓ Model loaded in {load_time:.1f} seconds!")
    
    
    def create_prompt(self, user_input):
        """
        Transform user input into a detailed prompt for SD Inpainting.
        
        PROMPT ENGINEERING FOR INPAINTING:
        Different from regular SD! We need to describe clothing that fits
        naturally on a person, not a standalone image.
        
        PARAMETERS:
        - user_input: What the user said (e.g., "flames", "roses")
        
        RETURNS:
        - prompt: Detailed description for SD
        - negative_prompt: What to avoid
        """
        
        # Build the prompt
        # Focus on how the clothing looks ON the person
        prompt = f"""elegant fashion clothing made of {user_input},
        worn by a person, haute couture style, detailed fabric texture,
        natural lighting, photorealistic, high quality, professional fashion photography"""
        
        # Negative prompt - what we DON'T want
        # These prevent common SD inpainting problems
        negative_prompt = """low quality, blurry, distorted, deformed, ugly,
        bad anatomy, extra limbs, disconnected, floating objects,
        text, watermark, signature, face distortion, warped clothing,
        unnatural proportions, artifacts, noise"""
        
        return prompt, negative_prompt
    
    
    def generate_clothing_inpainting(self, frame, mask, prompt, negative_prompt=None, seed=None):
        """
        Generate clothing using SD Inpainting.
        
        THIS IS WHERE THE MAGIC HAPPENS!
        SD looks at the frame, sees the mask, and paints clothing in that area.
        
        PARAMETERS:
        - frame: Original video frame (numpy array, BGR)
        - mask: Binary mask from Phase 3 (numpy array, 0 or 255)
        - prompt: Text describing the clothing
        - negative_prompt: What to avoid
        - seed: Random seed for reproducibility
        
        RETURNS:
        - inpainted_image: PIL Image with clothing generated
        """
        
        if self.pipeline is None:
            raise Exception("Model not loaded! Call load_model() first.")
        
        print(f"\nGenerating clothing...")
        print(f"Prompt: {prompt[:100]}...")
        
        start_time = time.time()
        
        # Convert frame to PIL Image (RGB)
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        image_pil = Image.fromarray(frame_rgb)
        
        # Convert mask to PIL Image
        # SD Inpainting expects: 255=paint here, 0=leave alone
        mask_pil = Image.fromarray(mask)
        
        # Ensure correct sizes (SD works best with 512x512)
        # We'll resize, process, then resize back
        original_size = image_pil.size
        target_size = (512, 512)
        
        image_resized = image_pil.resize(target_size, Image.LANCZOS)
        mask_resized = mask_pil.resize(target_size, Image.NEAREST)
        
        # Set random seed if provided
        if seed is not None:
            generator = torch.Generator(device=self.device).manual_seed(seed)
        else:
            generator = None
        
        # Run SD Inpainting
        # The model will:
        # 1. Look at the original image
        # 2. See where the mask is
        # 3. Generate clothing that fits naturally in that area
        with torch.no_grad():
            result = self.pipeline(
                prompt=prompt,
                negative_prompt=negative_prompt,
                image=image_resized,
                mask_image=mask_resized,
                num_inference_steps=self.num_inference_steps,
                guidance_scale=self.guidance_scale,
                generator=generator
            )
        
        # Get the generated image
        inpainted_image = result.images[0]
        
        # Resize back to original size
        inpainted_image = inpainted_image.resize(original_size, Image.LANCZOS)
        
        gen_time = time.time() - start_time
        print(f"✓ Generation complete in {gen_time:.1f} seconds!")
        
        return inpainted_image
    
    
    def extract_clothing_with_transparency(self, inpainted_image, mask):
        """
        Extract just the clothing from the inpainted image using the mask.
        
        This creates a PNG with transparency so you can overlay it on video.
        
        PARAMETERS:
        - inpainted_image: PIL Image with clothing generated
        - mask: Binary mask showing where clothing is (numpy array)
        
        RETURNS:
        - clothing_png: PIL Image (RGBA) with clothing only, transparent elsewhere
        """
        
        print("Extracting clothing with transparency...")
        
        # Convert inpainted image to numpy
        inpainted_array = np.array(inpainted_image)
        
        # Ensure mask is same size as image
        if mask.shape[:2] != inpainted_array.shape[:2]:
            mask = cv2.resize(mask, 
                            (inpainted_array.shape[1], inpainted_array.shape[0]),
                            interpolation=cv2.INTER_NEAREST)
        
        # Create RGBA image (RGB + Alpha channel)
        clothing_rgba = np.zeros(
            (inpainted_array.shape[0], inpainted_array.shape[1], 4),
            dtype=np.uint8
        )
        
        # Copy RGB channels
        clothing_rgba[:, :, :3] = inpainted_array
        
        # Set alpha channel from mask
        # Where mask is 255 (clothing area), alpha = 255 (opaque)
        # Where mask is 0 (background), alpha = 0 (transparent)
        clothing_rgba[:, :, 3] = mask
        
        # Convert to PIL Image
        clothing_png = Image.fromarray(clothing_rgba, mode='RGBA')
        
        print("✓ Clothing extracted with transparency")
        
        return clothing_png
    
    
    def generate_clothing_from_text(self, frame, mask, text, use_cache=True):
        """
        Main function: Generate clothing from text prompt.
        
        THIS IS WHAT YOU'LL CALL FROM YOUR MAIN APP!
        
        PARAMETERS:
        - frame: Video frame (numpy array, BGR)
        - mask: Body part mask from Phase 3 (numpy array, 0 or 255)
        - text: User's spoken text ("flames", "roses", etc.)
        - use_cache: Whether to check cache for this text
        
        RETURNS:
        - inpainted_full: PIL Image with full inpainted frame
        - clothing_png: PIL Image (RGBA) with just the clothing (transparent bg)
        
        RETURNS None, None if generation fails
        """
        
        # Check cache
        if use_cache and text in self.generation_cache:
            print(f"Using cached result for '{text}'")
            return self.generation_cache[text]
        
        try:
            # Create prompt
            prompt, negative_prompt = self.create_prompt(text)
            
            # Generate with inpainting
            inpainted_full = self.generate_clothing_inpainting(
                frame, mask, prompt, negative_prompt
            )
            
            # Extract clothing with transparency
            clothing_png = self.extract_clothing_with_transparency(
                inpainted_full, mask
            )
            
            # Cache the result
            if use_cache:
                self.generation_cache[text] = (inpainted_full, clothing_png)
            
            return inpainted_full, clothing_png
        
        except Exception as e:
            print(f"✗ Error generating clothing: {e}")
            import traceback
            traceback.print_exc()
            return None, None
    
    
    def save_images(self, inpainted_full, clothing_png, base_filename="generated"):
        """
        Save the generated images to disk.
        
        PARAMETERS:
        - inpainted_full: Full inpainted image
        - clothing_png: Clothing with transparency
        - base_filename: Base name for files
        
        Saves two files:
        - {base_filename}_full.png - Full inpainted frame
        - {base_filename}_clothing.png - Just the clothing with transparency
        """
        
        os.makedirs("generated_images", exist_ok=True)
        
        full_path = os.path.join("generated_images", f"{base_filename}_full.png")
        clothing_path = os.path.join("generated_images", f"{base_filename}_clothing.png")
        
        inpainted_full.save(full_path)
        clothing_png.save(clothing_path)
        
        print(f"✓ Saved: {full_path}")
        print(f"✓ Saved: {clothing_path}")
        
        return full_path, clothing_path
    
    
    def cleanup(self):
        """Clean up resources."""
        if self.pipeline is not None:
            del self.pipeline
            self.pipeline = None
            
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            print("ClothingGenerator cleaned up")


# ============================================================================
# USAGE EXAMPLE - Integration with Phase 3
# ============================================================================

def main():
    """
    Test the clothing generation with inpainting.
    This demonstrates the Phase 3 → Phase 4 pipeline.
    """
    
    print("=" * 60)
    print("PHASE 4: AI CLOTHING GENERATION TEST")
    print("=" * 60)
    print("\nThis will generate clothing using SD Inpainting.")
    print("You need Phase 3 working first!")
    print("=" * 60)
    
    # For testing, we'll create a simple test scenario
    # In the real app, you'll get frame and mask from Phase 3
    
    print("\nNOTE: This test requires you to have:")
    print("1. A test image with a person")
    print("2. A corresponding mask from Phase 3")
    print("\nFor now, we'll do a simplified test.")
    
    # Initialize generator
    generator = ClothingGenerator()
    
    # Load model
    try:
        generator.load_model()
    except Exception as e:
        print(f"\n✗ Error loading model: {e}")
        print("\nTROUBLESHOOTING:")
        print("1. Make sure you have enough disk space (~5GB)")
        print("2. Check your internet connection")
        print("3. Try again - sometimes downloads timeout")
        return
    
    # Test prompts
    test_prompts = [
        "flames",
        "roses and thorns",
        "water droplets",
        "golden scales",
        "starry night sky"
    ]
    
    print("\n" + "="*60)
    print("INTERACTIVE MODE")
    print("="*60)
    print("This test mode requires:")
    print("1. Run Phase 3 test first")
    print("2. Save a frame and mask using 'S' key in Phase 3")
    print("3. Place them in the project root as 'test_frame.png' and 'test_mask.png'")
    print("\nOnce you have those files, this will generate clothing!")
    print("="*60)
    
    # Check for test files
    if not os.path.exists('test_frame.png') or not os.path.exists('test_mask.png'):
        print("\n⚠ Test files not found!")
        print("Please run Phase 3 first and save a frame + mask.")
        print("Then rerun this test.")
        return
    
    # Load test frame and mask
    test_frame = cv2.imread('test_frame.png')
    test_mask = cv2.imread('test_mask.png', cv2.IMREAD_GRAYSCALE)
    
    print("\n✓ Test files loaded!")
    print(f"Frame size: {test_frame.shape}")
    print(f"Mask size: {test_mask.shape}")
    
    # Test with different prompts
    for i, prompt_text in enumerate(test_prompts):
        print(f"\n[{i+1}/{len(test_prompts)}] Generating: '{prompt_text}'")
        
        # Generate clothing
        inpainted_full, clothing_png = generator.generate_clothing_from_text(
            test_frame, test_mask, prompt_text
        )
        
        if inpainted_full is not None and clothing_png is not None:
            # Save the results
            generator.save_images(
                inpainted_full, clothing_png,
                base_filename=f"{i+1}_{prompt_text.replace(' ', '_')}"
            )
            
            print(f"✓ Success! Check generated_images folder")
        else:
            print(f"✗ Failed to generate")
        
        print("-" * 60)
    
    print("\n" + "="*60)
    print("TEST COMPLETE!")
    print("="*60)
    print("Check the 'generated_images' folder for results.")
    print("You should see:")
    print("- *_full.png files: Complete inpainted frames")
    print("- *_clothing.png files: Just the clothing with transparency")
    print("\nThe *_clothing.png files can be overlaid on live video!")
    
    # Cleanup
    generator.cleanup()


if __name__ == "__main__":
    main()


# ============================================================================
# TESTING CHECKLIST
# ============================================================================
#
# PREREQUISITES:
# [ ] Phase 3 is working
# [ ] You've saved test_frame.png and test_mask.png from Phase 3
#
# FIRST RUN:
# [ ] Model downloads successfully (~5GB)
# [ ] Model loads without errors
# [ ] Test generation completes
# [ ] Two PNG files saved per prompt
#
# CHECK RESULTS:
# [ ] *_full.png shows person wearing generated clothing
# [ ] Clothing fits naturally on the body
# [ ] *_clothing.png has transparency (checkerboard in image viewer)
# [ ] Clothing in PNG matches the masked area
# [ ] Multiple prompts generate different styles
#
# QUALITY CHECKS:
# [ ] Clothing looks realistic and detailed
# [ ] No weird artifacts or distortions
# [ ] Clothing edges blend smoothly with body
# [ ] Colors and textures match the prompt
#
# COMMON ISSUES:
# - Out of memory: Reduce num_inference_steps to 20
# - Slow generation: Normal on Mac CPU, expect 30-60 seconds
# - Poor quality: Increase guidance_scale to 9.0 or num_inference_steps to 50
# - Distorted clothing: Make sure mask is clean (check Phase 3)
# - Model download fails: Check internet, try again
#
# WHAT THIS GIVES YOU:
# ✓ Direct inpainting on the person (no warping needed)
# ✓ Natural-looking clothing that fits the pose
# ✓ Clothing PNG ready for live video overlay
# ✓ Fast iteration with caching
# ✓ Foundation for real-time application
#
# INTEGRATION WITH OTHER PHASES:
# - Phase 2: Text prompt comes from Whisper transcription
# - Phase 3: Mask comes from BodyPix segmentation
# - Phase 5: Clothing PNG gets overlaid on live video
# - Phase 8: All orchestrated in main application
#
# NEXT STEPS:
# - Test with different body poses
# - Try different clothing styles
# - Tune num_inference_steps and guidance_scale
# - Move to Phase 5 to overlay on live video
# - Integrate with Phase 2 for speech-to-clothing pipeline
#
# ============================================================================